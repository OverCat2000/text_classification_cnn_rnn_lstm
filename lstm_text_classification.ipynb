{"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPXM/jVXmBIck/Nr7bpWLXB","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c61a731c46d44d50b384db83a51f2508":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d65a71957d2447b59074ef681082f6f3","IPY_MODEL_3dbf38088c834927b291efc814387a57","IPY_MODEL_a1f484c7e6d2406f8de3d621a0f80241"],"layout":"IPY_MODEL_cd2cd7aa39ff4c80882e72a2d29967e3"}},"d65a71957d2447b59074ef681082f6f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5db913255e7f411992018b3430ae496f","placeholder":"​","style":"IPY_MODEL_4cd3f2a265254f9195d880693ddbdf1e","value":" 12%"}},"3dbf38088c834927b291efc814387a57":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e942537617f49148f3cc8e6db64e829","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1523909c899749248443adb5622bec99","value":12}},"a1f484c7e6d2406f8de3d621a0f80241":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e73badce0c346bb879c2f5089a4bf8e","placeholder":"​","style":"IPY_MODEL_d0cd9d1af91d41bcaaba0920be223b27","value":" 12/100 [08:35&lt;1:01:59, 42.27s/it]"}},"cd2cd7aa39ff4c80882e72a2d29967e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5db913255e7f411992018b3430ae496f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cd3f2a265254f9195d880693ddbdf1e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e942537617f49148f3cc8e6db64e829":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1523909c899749248443adb5622bec99":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e73badce0c346bb879c2f5089a4bf8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0cd9d1af91d41bcaaba0920be223b27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/OverCat2000/text_classification_cnn_rnn_lstm/blob/main/lstm_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"import gensim\nimport pathlib\nimport glob\nimport os\nfrom random import shuffle\nimport pickle\nfrom tqdm.auto import tqdm\n\nfrom nltk.tokenize import TreebankWordTokenizer\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn","metadata":{"id":"7RvNHbzcL7aa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a0246ef6-94fb-48ea-c552-99d717a7446a","execution":{"iopub.status.busy":"2024-06-21T07:31:44.345934Z","iopub.execute_input":"2024-06-21T07:31:44.346222Z","iopub.status.idle":"2024-06-21T07:31:57.808724Z","shell.execute_reply.started":"2024-06-21T07:31:44.346197Z","shell.execute_reply":"2024-06-21T07:31:57.807920Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!rm -r word2vec-google-news-300\n!git lfs install\n!git clone https://huggingface.co/fse/word2vec-google-news-300","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:34:08.531351Z","iopub.execute_input":"2024-06-21T07:34:08.531767Z","iopub.status.idle":"2024-06-21T07:37:38.508063Z","shell.execute_reply.started":"2024-06-21T07:34:08.531733Z","shell.execute_reply":"2024-06-21T07:37:38.506765Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Error: Failed to call git rev-parse --git-dir: exit status 128 \nGit LFS initialized.\nCloning into 'word2vec-google-news-300'...\nremote: Enumerating objects: 11, done.\u001b[K\nremote: Total 11 (delta 0), reused 0 (delta 0), pack-reused 11 (from 1)\u001b[K\nUnpacking objects: 100% (11/11), 1.51 KiB | 773.00 KiB/s, done.\nFiltering content: 100% (2/2), 3.52 GiB | 17.49 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n!tar -xf aclImdb_v1.tar.gz","metadata":{"id":"8IoccQC9sxC7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"31b3e1fa-63bd-4728-f7e6-066536710ee5","execution":{"iopub.status.busy":"2024-06-21T07:41:04.601763Z","iopub.execute_input":"2024-06-21T07:41:04.602791Z","iopub.status.idle":"2024-06-21T07:41:29.775669Z","shell.execute_reply.started":"2024-06-21T07:41:04.602753Z","shell.execute_reply":"2024-06-21T07:41:29.774435Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"--2024-06-21 07:41:05--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\nResolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\nConnecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 84125825 (80M) [application/x-gzip]\nSaving to: 'aclImdb_v1.tar.gz.1'\n\naclImdb_v1.tar.gz.1 100%[===================>]  80.23M  6.49MB/s    in 18s     \n\n2024-06-21 07:41:23 (4.53 MB/s) - 'aclImdb_v1.tar.gz.1' saved [84125825/84125825]\n\n\ngzip: stdin: unexpected end of file\ntar: Unexpected EOF in archive\ntar: Unexpected EOF in archive\ntar: Error is not recoverable: exiting now\n","output_type":"stream"}]},{"cell_type":"code","source":"model_path = '/kaggle/working/word2vec-google-news-300/word2vec-google-news-300.model'\n\ntrain_path = pathlib.Path('aclImdb/train')\npos_path = train_path / 'pos'\nneg_path = train_path / 'neg'\n\nfixed_len = 600\nbatch_size = 32\nembedding_dims = 300\nfilters = 250\nkernel_size = 3\nhidden_dims = 250\nepochs = 2\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bOeQ2Iz-xwN0","outputId":"afadd8d1-320a-49bb-a0a6-f21667e2cfe2","execution":{"iopub.status.busy":"2024-06-21T07:41:29.779095Z","iopub.execute_input":"2024-06-21T07:41:29.780153Z","iopub.status.idle":"2024-06-21T07:41:29.788637Z","shell.execute_reply.started":"2024-06-21T07:41:29.780103Z","shell.execute_reply":"2024-06-21T07:41:29.787687Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"There are 1 GPU(s) available.\nDevice name: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"# model = gensim.models.KeyedVectors.load(model_path)","metadata":{"id":"0j55lnZTABsG","execution":{"iopub.status.busy":"2024-06-21T07:41:29.790130Z","iopub.execute_input":"2024-06-21T07:41:29.790469Z","iopub.status.idle":"2024-06-21T07:41:29.798433Z","shell.execute_reply.started":"2024-06-21T07:41:29.790442Z","shell.execute_reply":"2024-06-21T07:41:29.797592Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def data(file_path):\n\n  dataset = []\n  pos_path = file_path / 'pos'\n  neg_path = file_path / 'neg'\n\n  for filename in glob.glob(os.path.join(pos_path, '*.txt')):\n      with open(filename, 'r') as f:\n        dataset.append((1, f.read()))\n\n  for filename in glob.glob(os.path.join(neg_path, '*.txt')):\n      with open(filename, 'r') as f:\n        dataset.append((0, f.read()))\n\n  shuffle(dataset)\n\n  return dataset","metadata":{"id":"G6lRz7SUx15S","execution":{"iopub.status.busy":"2024-06-21T07:41:29.800572Z","iopub.execute_input":"2024-06-21T07:41:29.800918Z","iopub.status.idle":"2024-06-21T07:41:29.809054Z","shell.execute_reply.started":"2024-06-21T07:41:29.800884Z","shell.execute_reply":"2024-06-21T07:41:29.808194Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"dataset = data(train_path)","metadata":{"id":"FEol5fCqx8cp","execution":{"iopub.status.busy":"2024-06-21T07:59:44.923770Z","iopub.execute_input":"2024-06-21T07:59:44.924528Z","iopub.status.idle":"2024-06-21T07:59:45.733499Z","shell.execute_reply.started":"2024-06-21T07:59:44.924494Z","shell.execute_reply":"2024-06-21T07:59:45.732410Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"len(dataset)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Qz5JwALPK8b","outputId":"eb3a6f6b-f1a4-4238-811b-fb9b1e45a671","execution":{"iopub.status.busy":"2024-06-21T07:59:45.735420Z","iopub.execute_input":"2024-06-21T07:59:45.735786Z","iopub.status.idle":"2024-06-21T07:59:45.742074Z","shell.execute_reply.started":"2024-06-21T07:59:45.735753Z","shell.execute_reply":"2024-06-21T07:59:45.741099Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"25000"},"metadata":{}}]},{"cell_type":"code","source":"def tokenizer(text):\n    tokenizer = TreebankWordTokenizer()\n    return tokenizer.tokenize(text)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T08:04:35.591363Z","iopub.execute_input":"2024-06-21T08:04:35.592330Z","iopub.status.idle":"2024-06-21T08:04:35.596383Z","shell.execute_reply.started":"2024-06-21T08:04:35.592277Z","shell.execute_reply":"2024-06-21T08:04:35.595501Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"def vocabulary(dataset):\n  #tokenizer = TreebankWordTokenizer()\n\n  max_len = 0\n  fixed_len = 600\n  tokenized_texts = []\n  word2idx = {}\n\n  word2idx['<PAD>'] = 0\n  word2idx['<UNK>'] = 1\n\n  idx = 2\n\n  for text in dataset:\n    tokenized_text = tokenizer(text[1])\n    tokenized_texts.append(tokenized_text)\n\n    for token in tokenized_text:\n\n      if token not in word2idx:\n        word2idx[token] = idx\n        idx = idx + 1\n\n    max_len = max(max_len, len(tokenized_text))\n\n  return max_len, word2idx, tokenized_texts","metadata":{"id":"QryYRU_gvQHX","execution":{"iopub.status.busy":"2024-06-21T08:04:35.800486Z","iopub.execute_input":"2024-06-21T08:04:35.801365Z","iopub.status.idle":"2024-06-21T08:04:35.811090Z","shell.execute_reply.started":"2024-06-21T08:04:35.801330Z","shell.execute_reply":"2024-06-21T08:04:35.810043Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"def encoder(fixed_len, tokenized_texts, word2idx):\n  input_ids = []\n\n  for tokenized_text in tokenized_texts:\n\n    if len(tokenized_text) < fixed_len:\n\n      tokenized_text += ['<PAD>']*(fixed_len - len(tokenized_text))\n\n    elif len(tokenized_text) > fixed_len:\n\n      tokenized_text = tokenized_text[:fixed_len]\n\n    input_id = [word2idx.get(i) for i in tokenized_text]\n    input_ids.append(input_id)\n\n  return np.array(input_ids)","metadata":{"id":"Qt8IwJVI1Hbk","execution":{"iopub.status.busy":"2024-06-21T08:04:39.021133Z","iopub.execute_input":"2024-06-21T08:04:39.021994Z","iopub.status.idle":"2024-06-21T08:04:39.028104Z","shell.execute_reply.started":"2024-06-21T08:04:39.021960Z","shell.execute_reply":"2024-06-21T08:04:39.027135Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"def load_pretrained_vectors(model_path, word2idx):\n  model = gensim.models.KeyedVectors.load(model_path)\n  print(\"model loaded\")\n  d = model.vector_size\n  embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n  available_count = 0\n  unavailabel_count = 0\n\n  if '<PAD>' in word2idx:\n    embeddings[word2idx['<PAD>']] = np.zeros(d)\n\n  for word, idx in word2idx.items():\n\n    try:\n      pretrained_embedddings = model[word]\n      available_count += 1\n      embeddings[idx] = pretrained_embedddings\n    except KeyError:\n      unavailabel_count +=1\n      pass\n\n  print(f\"availabel words: {available_count / len(word2idx) * 100}\")\n\n  return embeddings","metadata":{"id":"tQlksti46Spa","execution":{"iopub.status.busy":"2024-06-21T08:04:39.200798Z","iopub.execute_input":"2024-06-21T08:04:39.201185Z","iopub.status.idle":"2024-06-21T08:04:39.208891Z","shell.execute_reply.started":"2024-06-21T08:04:39.201154Z","shell.execute_reply":"2024-06-21T08:04:39.207928Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"def Labels(dataset):\n\n    labels = []\n    for sample in dataset:\n      labels.append(sample[0])\n    return np.array(labels)","metadata":{"id":"ReW2qBsWGJWY","execution":{"iopub.status.busy":"2024-06-21T08:04:39.511034Z","iopub.execute_input":"2024-06-21T08:04:39.511650Z","iopub.status.idle":"2024-06-21T08:04:39.516351Z","shell.execute_reply.started":"2024-06-21T08:04:39.511616Z","shell.execute_reply":"2024-06-21T08:04:39.515402Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"def main(model_path, dataset):\n  max_len, word2idx, tokenized_texts = vocabulary(dataset)\n  input_ids = encoder(fixed_len, tokenized_texts, word2idx)\n\n  embeddings = load_pretrained_vectors(model_path, word2idx)\n  embeddings = torch.tensor(embeddings)\n\n  labels = Labels(dataset)\n  train_size = int(len(input_ids) * 0.8)\n\n  X_train = torch.Tensor(input_ids[:train_size])\n  X_val = torch.Tensor(input_ids[train_size:])\n\n  y_train = torch.Tensor(labels[:train_size])\n  y_val = torch.Tensor(labels[train_size:])\n\n  train_dataloader = DataLoader(TensorDataset(X_train, y_train), batch_size, True)\n  val_dataloader = DataLoader(TensorDataset(X_val, y_val), batch_size, False)\n\n  return train_dataloader, val_dataloader, embeddings, word2idx","metadata":{"id":"he69CHGgzqMc","execution":{"iopub.status.busy":"2024-06-21T08:04:39.840970Z","iopub.execute_input":"2024-06-21T08:04:39.841636Z","iopub.status.idle":"2024-06-21T08:04:39.848940Z","shell.execute_reply.started":"2024-06-21T08:04:39.841603Z","shell.execute_reply":"2024-06-21T08:04:39.847914Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"train_dataloader, val_dataloader, embeddings, vocab = main(model_path, dataset)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Srbgb6k2GVtB","outputId":"509042d7-c18c-41c0-92f3-bd44bbe142c4","execution":{"iopub.status.busy":"2024-06-21T08:04:40.161142Z","iopub.execute_input":"2024-06-21T08:04:40.162048Z","iopub.status.idle":"2024-06-21T08:06:12.935444Z","shell.execute_reply.started":"2024-06-21T08:04:40.162013Z","shell.execute_reply":"2024-06-21T08:06:12.934586Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"model loaded\navailabel words: 51.5337586390218\n","output_type":"stream"}]},{"cell_type":"code","source":"class LSTMnet(nn.Module):\n  def __init__(self, hidden_units, output_shape, kernel, pretrained_embeddings):\n    super().__init__()\n\n    self.vocab_size, self.embed_dim = pretrained_embeddings.shape\n    self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n\n    # self.block = nn.Sequential(\n    #     nn.Conv1d(self.embed_dim, hidden_units, kernel_size=kernel),\n    #     nn.ReLU(),\n    #     nn.MaxPool1d(2)\n    # )\n\n    #self.rnn = nn.RNN(input_size=self.embed_dim, hidden_size=hidden_units, batch_first=True, bidirectional=True)\n    self.lstm = nn.LSTM(input_size=self.embed_dim, hidden_size=hidden_units, batch_first=True, bidirectional=True)\n    \n    self.drop = nn.Dropout(0.5)\n\n\n\n    self.flat = nn.Flatten(start_dim=1)\n    self.fc = nn.Linear(in_features=hidden_units*1200, out_features=output_shape)\n    #self.sigmoid = nn.Sigmoid()\n\n\n  def forward(self, x):\n    x = self.embedding(x.int()).float()\n    x, _ = self.lstm(x)\n    x = self.drop(x)\n    x = self.flat(x)\n    x = self.fc(x)\n    #x = self.sigmoid(x)\n    return x","metadata":{"id":"3Mqrc9CU7rOL","execution":{"iopub.status.busy":"2024-06-21T07:43:36.566308Z","iopub.execute_input":"2024-06-21T07:43:36.567123Z","iopub.status.idle":"2024-06-21T07:43:36.576061Z","shell.execute_reply.started":"2024-06-21T07:43:36.567091Z","shell.execute_reply":"2024-06-21T07:43:36.574847Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class BinaryAccuracy:\n    def __init__(self, threshold=0.5):\n        self.threshold = threshold\n\n    def __call__(self, logits, targets):\n        # Apply sigmoid to logits to get probabilities\n        probabilities = torch.sigmoid(logits).squeeze(dim=1)\n        # Convert probabilities to binary predictions\n        predictions = (probabilities >= self.threshold).float()\n        # Compare predictions with targets and calculate accuracy\n        correct = (predictions == targets).float().sum()\n        accuracy = correct / targets.numel()\n        return accuracy.item()","metadata":{"id":"zjMnKvQt7rK6","execution":{"iopub.status.busy":"2024-06-21T07:43:37.075933Z","iopub.execute_input":"2024-06-21T07:43:37.076307Z","iopub.status.idle":"2024-06-21T07:43:37.082808Z","shell.execute_reply.started":"2024-06-21T07:43:37.076279Z","shell.execute_reply":"2024-06-21T07:43:37.081850Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"lstm = LSTMnet(filters, 1, kernel_size, embeddings)\nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(params=lstm.parameters(), lr=0.001)\naccuracy_fn = BinaryAccuracy()\nlstm.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fD0q9lA8_NJ3","outputId":"795135e6-84f9-45dc-f589-f617acbbaf06","execution":{"iopub.status.busy":"2024-06-21T07:43:37.301178Z","iopub.execute_input":"2024-06-21T07:43:37.301935Z","iopub.status.idle":"2024-06-21T07:43:38.686266Z","shell.execute_reply.started":"2024-06-21T07:43:37.301900Z","shell.execute_reply":"2024-06-21T07:43:38.685237Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"LSTMnet(\n  (embedding): Embedding(150480, 300)\n  (lstm): LSTM(300, 250, batch_first=True, bidirectional=True)\n  (drop): Dropout(p=0.5, inplace=False)\n  (flat): Flatten(start_dim=1, end_dim=-1)\n  (fc): Linear(in_features=300000, out_features=1, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def train_step(model: torch.nn.Module,\n               dataloader: torch.utils.data.DataLoader,\n               loss_fn: torch.nn.Module,\n               optimizer: torch.optim.Optimizer):\n    # Put model in train mode\n    model.train()\n\n    # Setup train loss and train accuracy values\n    train_loss, train_acc = 0, 0\n\n    # Loop through data loader data batches\n    for batch, (X, y) in enumerate(dataloader):\n        # 1. Forward pass\n        X = X.to(device)\n        y = y.to(device)\n        y_pred = model(X)\n\n\n\n        # 2. Calculate  and accumulate loss\n        loss = loss_fn(y_pred.squeeze(1), y)\n        train_loss += loss.item()\n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Calculate and accumulate accuracy metric across all batches\n        train_acc += accuracy_fn(y_pred, y)\n\n    # Adjust metrics to get average loss and accuracy per batch\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc","metadata":{"id":"_PJ0DVn_E486","execution":{"iopub.status.busy":"2024-06-21T07:43:38.688426Z","iopub.execute_input":"2024-06-21T07:43:38.688869Z","iopub.status.idle":"2024-06-21T07:43:38.698003Z","shell.execute_reply.started":"2024-06-21T07:43:38.688830Z","shell.execute_reply":"2024-06-21T07:43:38.696887Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def test_step(model: torch.nn.Module,\n              dataloader: torch.utils.data.DataLoader,\n              loss_fn: torch.nn.Module):\n    # Put model in eval mode\n    model.eval()\n\n    # Setup test loss and test accuracy values\n    test_loss, test_acc = 0, 0\n\n    # Turn on inference context manager\n    with torch.inference_mode():\n        # Loop through DataLoader batches\n        for batch, (X, y) in enumerate(dataloader):\n            X = X.to(device)\n            y = y.to(device)\n            # 1. Forward pass\n            test_pred_logits = model(X)\n\n            # 2. Calculate and accumulate loss\n            loss = loss_fn(test_pred_logits.squeeze(1), y)\n            test_loss += loss.item()\n\n            # Calculate and accumulate accuracy\n            test_acc += accuracy_fn(test_pred_logits, y)\n\n    # Adjust metrics to get average loss and accuracy per batch\n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    return test_loss, test_acc","metadata":{"id":"9TfItmWfMGEQ","execution":{"iopub.status.busy":"2024-06-21T07:43:38.699089Z","iopub.execute_input":"2024-06-21T07:43:38.699400Z","iopub.status.idle":"2024-06-21T07:43:38.714357Z","shell.execute_reply.started":"2024-06-21T07:43:38.699374Z","shell.execute_reply":"2024-06-21T07:43:38.713299Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def train(model: torch.nn.Module,\n          train_dataloader: torch.utils.data.DataLoader,\n          test_dataloader: torch.utils.data.DataLoader,\n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int = 5):\n\n    # 2. Create empty results dictionary\n    results = {\"train_loss\": [],\n        \"train_acc\": [],\n        \"test_loss\": [],\n        \"test_acc\": []\n    }\n\n    # 3. Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer)\n        test_loss, test_acc = test_step(model=model,\n            dataloader=test_dataloader,\n            loss_fn=loss_fn)\n\n        # 4. Print out what's happening\n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n            f\"test_loss: {test_loss:.4f} | \"\n            f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # 5. Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n    # 6. Return the filled results at the end of the epochs\n    return results","metadata":{"id":"KCxdApiNMGHg","execution":{"iopub.status.busy":"2024-06-21T07:43:38.756535Z","iopub.execute_input":"2024-06-21T07:43:38.757284Z","iopub.status.idle":"2024-06-21T07:43:38.765883Z","shell.execute_reply.started":"2024-06-21T07:43:38.757253Z","shell.execute_reply":"2024-06-21T07:43:38.764796Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Set random seeds\ntorch.manual_seed(42)\n\n# Set number of epochs\nNUM_EPOCHS = 10\n\n# Start the timer\nfrom timeit import default_timer as timer\nstart_time = timer()\n\n# Train model_0\nmodel_0_results = train(model=lstm,\n                        train_dataloader=train_dataloader,\n                        test_dataloader=val_dataloader,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn,\n                        epochs=NUM_EPOCHS)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":571,"referenced_widgets":["c61a731c46d44d50b384db83a51f2508","d65a71957d2447b59074ef681082f6f3","3dbf38088c834927b291efc814387a57","a1f484c7e6d2406f8de3d621a0f80241","cd2cd7aa39ff4c80882e72a2d29967e3","5db913255e7f411992018b3430ae496f","4cd3f2a265254f9195d880693ddbdf1e","3e942537617f49148f3cc8e6db64e829","1523909c899749248443adb5622bec99","7e73badce0c346bb879c2f5089a4bf8e","d0cd9d1af91d41bcaaba0920be223b27"]},"id":"YqkHj5PYMGKQ","outputId":"4ac4dabb-30d9-4274-bb9f-d6b51bbffa59","execution":{"iopub.status.busy":"2024-06-21T08:13:35.375860Z","iopub.execute_input":"2024-06-21T08:13:35.376262Z","iopub.status.idle":"2024-06-21T08:20:35.450242Z","shell.execute_reply.started":"2024-06-21T08:13:35.376233Z","shell.execute_reply":"2024-06-21T08:20:35.448805Z"},"trusted":true},"execution_count":103,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eafe525eb6d411f8a47756bc859ffcf"}},"metadata":{}},{"name":"stdout","text":"Epoch: 1 | train_loss: 0.4615 | train_acc: 0.7905 | test_loss: 0.3235 | test_acc: 0.8666\nEpoch: 2 | train_loss: 0.0600 | train_acc: 0.9806 | test_loss: 0.3752 | test_acc: 0.8692\nEpoch: 3 | train_loss: 0.0076 | train_acc: 0.9988 | test_loss: 0.4801 | test_acc: 0.8718\nEpoch: 4 | train_loss: 0.0018 | train_acc: 0.9999 | test_loss: 0.4622 | test_acc: 0.8746\nEpoch: 5 | train_loss: 0.0008 | train_acc: 0.9999 | test_loss: 0.5364 | test_acc: 0.8762\nEpoch: 6 | train_loss: 0.0004 | train_acc: 1.0000 | test_loss: 0.7447 | test_acc: 0.8712\nEpoch: 7 | train_loss: 0.0001 | train_acc: 1.0000 | test_loss: 0.6899 | test_acc: 0.8786\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[103], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m start_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train model_0\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m model_0_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# End the timer and print out how long it took\u001b[39;00m\n\u001b[1;32m     20\u001b[0m end_time \u001b[38;5;241m=\u001b[39m timer()\n","Cell \u001b[0;32mIn[35], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 3. Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m---> 17\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_step(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     22\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[1;32m     23\u001b[0m         loss_fn\u001b[38;5;241m=\u001b[39mloss_fn)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# 4. Print out what's happening\u001b[39;00m\n","Cell \u001b[0;32mIn[33], line 22\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 2. Calculate  and accumulate loss\u001b[39;00m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), y)\n\u001b[0;32m---> 22\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 3. Optimizer zero grad\u001b[39;00m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"def make_predictions(model, data):\n\n    pred_class = []\n\n    model.eval()\n\n    with torch.inference_mode():\n        for sample in data:\n            sample = torch.unsqueeze(sample, dim=0)\n            sample = sample.to(device)\n\n            pred_logits = model(sample)\n\n            pred_probs = torch.sigmoid(pred_logits)\n\n            if pred_probs >= 0.5:\n                pred_class.append(1)\n            else:\n                pred_class.append(0)\n\n    return pred_class","metadata":{"id":"Cq8QCWm6MGNI","execution":{"iopub.status.busy":"2024-06-21T08:20:38.995697Z","iopub.execute_input":"2024-06-21T08:20:38.996090Z","iopub.status.idle":"2024-06-21T08:20:39.002327Z","shell.execute_reply.started":"2024-06-21T08:20:38.996060Z","shell.execute_reply":"2024-06-21T08:20:39.001320Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"sample_labels = next(iter(val_dataloader))[1]\nsample_features = next(iter(val_dataloader))[0]\n\npred = make_predictions(lstm, sample_features)\n\nprint(pred), print(sample_labels.int().tolist())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R-ZViomWMGPq","outputId":"f34976c0-78ec-427d-e95a-1a78fdbe0ad2","execution":{"iopub.status.busy":"2024-06-21T08:20:39.200692Z","iopub.execute_input":"2024-06-21T08:20:39.201063Z","iopub.status.idle":"2024-06-21T08:20:39.723848Z","shell.execute_reply.started":"2024-06-21T08:20:39.201031Z","shell.execute_reply":"2024-06-21T08:20:39.722917Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"[1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1]\n[1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1]\n","output_type":"stream"},{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"(None, None)"},"metadata":{}}]},{"cell_type":"code","source":"sample = [(0, \"the movies was about a real life amazing story about a person.it was amazing\")]\n\nsample_tokens = [tokenizer(i[1]) for i in sample]\n#print(sample_tokens)\n\ndata = np.array(encoder(600, sample_tokens, vocab), dtype=\"float32\")\n\ndata = torch.Tensor(data)\n\nmake_predictions(lstm, data)","metadata":{"id":"HsHbpSN-_NDf","execution":{"iopub.status.busy":"2024-06-21T08:29:19.170282Z","iopub.execute_input":"2024-06-21T08:29:19.171146Z","iopub.status.idle":"2024-06-21T08:29:19.197812Z","shell.execute_reply.started":"2024-06-21T08:29:19.171115Z","shell.execute_reply":"2024-06-21T08:29:19.196906Z"},"trusted":true},"execution_count":140,"outputs":[{"execution_count":140,"output_type":"execute_result","data":{"text/plain":"[1]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"5l9fiQ1b50Vd"},"execution_count":null,"outputs":[]}]}